<!DOCTYPE html>
<html>
<head>
    <meta http-eqiv='content-type' content='text/html;charset=utf-8'>
    <title>ceph</title>
    <link rel=stylesheet href="http://jashkenas.github.com/docco/resources/docco.css">
</head>
<body>
<div id=container>
    <div id=background></div>
    <table cellspacing=0 cellpadding=0>
    <thead>
      <tr>
        <th class=docs><h1>ceph</h1></th>
        <th class=code></th>
      </tr>
    </thead>
    <tbody>
        <tr><td class='docs'><?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.11: http://docutils.sourceforge.net/" />
<title></title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document">


<p>lib/ceph
Functions to control the configuration and operation of the <strong>Ceph</strong> storage service
Dependencies:</p>
<ul class="simple">
<li><tt class="docutils literal">functions</tt> file</li>
<li><tt class="docutils literal">CEPH_DATA_DIR</tt> or <tt class="docutils literal">DATA_DIR</tt> must be defined</li>
</ul>
</td><td class=code><div class=highlight><pre>


</pre></div></td></tr><tr><td class=docs>
<p><tt class="docutils literal">stack.sh</tt> calls the entry points in this order (via <tt class="docutils literal"><span class="pre">extras.d/60-ceph.sh</span></tt>):</p>
<ul class="simple">
<li>install_ceph</li>
<li>configure_ceph</li>
<li>init_ceph</li>
<li>start_ceph</li>
<li>stop_ceph</li>
<li>cleanup_ceph</li>
</ul>
</td><td class=code><div class=highlight><pre>

</pre></div></td></tr><tr><td class=docs>
<p>Save trace setting</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">XTRACE</span><span class="o">=</span><span class="k">$(</span><span class="nb">set</span> +o | grep xtrace<span class="k">)</span>
<span class="nb">set</span> +o xtrace


</pre></div></td></tr><tr><td class=docs>
<div class="section" id="defaults">
<h1>Defaults</h1>
</td><td class=code><div class=highlight><pre>

</pre></div></td></tr><tr><td class=docs>
<p>Set <tt class="docutils literal">CEPH_DATA_DIR</tt> to the location of Ceph drives and objects.
Default is the common DevStack data directory.</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">CEPH_DATA_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">:-</span><span class="p">/var/lib/ceph</span><span class="k">}</span>
<span class="nv">CEPH_DISK_IMAGE</span><span class="o">=</span><span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span>/drives/images/ceph.img

</pre></div></td></tr><tr><td class=docs>
<p>Set <tt class="docutils literal">CEPH_CONF_DIR</tt> to the location of the configuration files.
Default is <tt class="docutils literal">/etc/ceph</tt>.</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">CEPH_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">CEPH_CONF_DIR</span><span class="k">:-</span><span class="p">/etc/ceph</span><span class="k">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>DevStack will create a loop-back disk formatted as XFS to store the
Ceph data. Set <tt class="docutils literal">CEPH_LOOPBACK_DISK_SIZE</tt> to the disk size in
kilobytes.
Default is 1 gigabyte.</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">CEPH_LOOPBACK_DISK_SIZE_DEFAULT</span><span class="o">=</span>2G
<span class="nv">CEPH_LOOPBACK_DISK_SIZE</span><span class="o">=</span><span class="k">${</span><span class="nv">CEPH_LOOPBACK_DISK_SIZE</span><span class="k">:-</span><span class="nv">$CEPH_LOOPBACK_DISK_SIZE_DEFAULT</span><span class="k">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>Common</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">CEPH_FSID</span><span class="o">=</span><span class="k">$(</span>uuidgen<span class="k">)</span>
<span class="nv">CEPH_CONF_FILE</span><span class="o">=</span><span class="k">${</span><span class="nv">CEPH_CONF_DIR</span><span class="k">}</span>/ceph.conf

</pre></div></td></tr><tr><td class=docs>
<p>Glance</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">GLANCE_CEPH_USER</span><span class="o">=</span><span class="k">${</span><span class="nv">GLANCE_CEPH_USER</span><span class="k">:-</span><span class="nv">glance</span><span class="k">}</span>
<span class="nv">GLANCE_CEPH_POOL</span><span class="o">=</span><span class="k">${</span><span class="nv">GLANCE_CEPH_POOL</span><span class="k">:-</span><span class="nv">images</span><span class="k">}</span>
<span class="nv">GLANCE_CEPH_POOL_PG</span><span class="o">=</span><span class="k">${</span><span class="nv">GLANCE_CEPH_POOL_PG</span><span class="k">:-</span><span class="nv">8</span><span class="k">}</span>
<span class="nv">GLANCE_CEPH_POOL_PGP</span><span class="o">=</span><span class="k">${</span><span class="nv">GLANCE_CEPH_POOL_PGP</span><span class="k">:-</span><span class="nv">8</span><span class="k">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>Nova</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">NOVA_CEPH_POOL</span><span class="o">=</span><span class="k">${</span><span class="nv">NOVA_CEPH_POOL</span><span class="k">:-</span><span class="nv">vms</span><span class="k">}</span>
<span class="nv">NOVA_CEPH_POOL_PG</span><span class="o">=</span><span class="k">${</span><span class="nv">NOVA_CEPH_POOL_PG</span><span class="k">:-</span><span class="nv">8</span><span class="k">}</span>
<span class="nv">NOVA_CEPH_POOL_PGP</span><span class="o">=</span><span class="k">${</span><span class="nv">NOVA_CEPH_POOL_PGP</span><span class="k">:-</span><span class="nv">8</span><span class="k">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>Cinder</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">CINDER_CEPH_POOL</span><span class="o">=</span><span class="k">${</span><span class="nv">CINDER_CEPH_POOL</span><span class="k">:-</span><span class="nv">volumes</span><span class="k">}</span>
<span class="nv">CINDER_CEPH_POOL_PG</span><span class="o">=</span><span class="k">${</span><span class="nv">CINDER_CEPH_POOL_PG</span><span class="k">:-</span><span class="nv">8</span><span class="k">}</span>
<span class="nv">CINDER_CEPH_POOL_PGP</span><span class="o">=</span><span class="k">${</span><span class="nv">CINDER_CEPH_POOL_PGP</span><span class="k">:-</span><span class="nv">8</span><span class="k">}</span>
<span class="nv">CINDER_CEPH_USER</span><span class="o">=</span><span class="k">${</span><span class="nv">CINDER_CEPH_USER</span><span class="k">:-</span><span class="nv">cinder</span><span class="k">}</span>
<span class="nv">CINDER_CEPH_UUID</span><span class="o">=</span><span class="k">${</span><span class="nv">CINDER_CEPH_UUID</span><span class="k">:-$(</span>uuidgen<span class="k">)}</span>

</pre></div></td></tr><tr><td class=docs>
<p>Set <tt class="docutils literal">CEPH_REPLICAS</tt> to configure how many replicas are to be
configured for your Ceph cluster. By default we are configuring
only one replica since this is way less CPU and memory intensive. If
you are planning to test Ceph replication feel free to increase this value</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">CEPH_REPLICAS</span><span class="o">=</span><span class="k">${</span><span class="nv">CEPH_REPLICAS</span><span class="k">:-</span><span class="nv">1</span><span class="k">}</span>
<span class="nv">CEPH_REPLICAS_SEQ</span><span class="o">=</span><span class="k">$(</span>seq <span class="k">${</span><span class="nv">CEPH_REPLICAS</span><span class="k">})</span>

</pre></div></td></tr><tr><td class=docs>
</div>
<div class="section" id="functions">
<h1>Functions</h1>
</td><td class=code><div class=highlight><pre>

</pre></div></td></tr><tr><td class=docs>
<p>import_libvirt_secret_ceph() - Imports Cinder user key into libvirt
so it can connect to the Ceph cluster while attaching a Cinder block device</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>import_libvirt_secret_ceph <span class="o">{</span>
    cat &gt; secret.xml <span class="s">&lt;&lt;EOF</span>
<span class="s">&lt;secret ephemeral=&#39;no&#39; private=&#39;no&#39;&gt;</span>
<span class="s">   &lt;uuid&gt;${CINDER_CEPH_UUID}&lt;/uuid&gt;</span>
<span class="s">   &lt;usage type=&#39;ceph&#39;&gt;</span>
<span class="s">     &lt;name&gt;client.${CINDER_CEPH_USER} secret&lt;/name&gt;</span>
<span class="s">   &lt;/usage&gt;</span>
<span class="s">&lt;/secret&gt;</span>
<span class="s">EOF</span>
    sudo virsh secret-define --file secret.xml
    sudo virsh secret-set-value --secret <span class="k">${</span><span class="nv">CINDER_CEPH_UUID</span><span class="k">}</span> --base64 <span class="k">$(</span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> auth get-key client.<span class="k">${</span><span class="nv">CINDER_CEPH_USER</span><span class="k">})</span>
    sudo rm -f secret.xml
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>cleanup_ceph() - Remove residual data files, anything left over from previous
runs that a clean run would need to clean up</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>cleanup_ceph <span class="o">{</span>
    sudo pkill -f ceph-mon
    sudo pkill -f ceph-osd
    sudo rm -rf <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span>/*/*
    sudo rm -rf <span class="k">${</span><span class="nv">CEPH_CONF_DIR</span><span class="k">}</span>/*
    <span class="k">if </span>egrep -q <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span> /proc/mounts; <span class="k">then</span>
<span class="k">        </span>sudo umount <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span>
    <span class="k">fi</span>
<span class="k">    if</span> <span class="o">[[</span> -e <span class="k">${</span><span class="nv">CEPH_DISK_IMAGE</span><span class="k">}</span> <span class="o">]]</span>; <span class="k">then</span>
<span class="k">        </span>sudo rm -f <span class="k">${</span><span class="nv">CEPH_DISK_IMAGE</span><span class="k">}</span>
    <span class="k">fi</span>
<span class="k">    </span>uninstall_package ceph ceph-common python-ceph libcephfs1 &gt; /dev/null 2&gt;&amp;1
    <span class="nv">VIRSH_UUID</span><span class="o">=</span><span class="k">$(</span>sudo virsh secret-list | awk <span class="s1">&#39;/^ ?[0-9a-z]/ { print $1 }&#39;</span><span class="k">)</span>
    sudo virsh secret-undefine <span class="k">${</span><span class="nv">VIRSH_UUID</span><span class="k">}</span> &gt;/dev/null 2&gt;&amp;1
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>configure_ceph() - Set config files, create data dirs, etc</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>configure_ceph <span class="o">{</span>
    <span class="nb">local </span><span class="nv">count</span><span class="o">=</span>0

</pre></div></td></tr><tr><td class=docs>
<p>create a backing file disk</p>
</td><td class=code><div class=highlight><pre>
    create_disk <span class="k">${</span><span class="nv">CEPH_DISK_IMAGE</span><span class="k">}</span> <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span> <span class="k">${</span><span class="nv">CEPH_LOOPBACK_DISK_SIZE</span><span class="k">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>populate ceph directory</p>
</td><td class=code><div class=highlight><pre>
    sudo mkdir -p <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span>/<span class="o">{</span>bootstrap-mds,bootstrap-osd,mds,mon,osd,tmp<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>create ceph monitor initial key and directory</p>
</td><td class=code><div class=highlight><pre>
    sudo ceph-authtool /var/lib/ceph/tmp/keyring.mon.<span class="k">$(</span>hostname<span class="k">)</span> --create-keyring --name<span class="o">=</span>mon. --add-key<span class="o">=</span><span class="k">$(</span>ceph-authtool --gen-print-key<span class="k">)</span> --cap mon <span class="s1">&#39;allow *&#39;</span>
    sudo mkdir /var/lib/ceph/mon/ceph-<span class="k">$(</span>hostname<span class="k">)</span>

</pre></div></td></tr><tr><td class=docs>
<p>create a default ceph configuration file</p>
</td><td class=code><div class=highlight><pre>
    sudo tee -a <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> &gt; /dev/null <span class="s">&lt;&lt;EOF</span>
<span class="s">[global]</span>
<span class="s">fsid = ${CEPH_FSID}</span>
<span class="s">mon_initial_members = $(hostname)</span>
<span class="s">mon_host = ${SERVICE_HOST}</span>
<span class="s">auth_cluster_required = cephx</span>
<span class="s">auth_service_required = cephx</span>
<span class="s">auth_client_required = cephx</span>
<span class="s">filestore_xattr_use_omap = true</span>
<span class="s">osd crush chooseleaf type = 0</span>
<span class="s">osd journal size = 100</span>
<span class="s">EOF</span>

</pre></div></td></tr><tr><td class=docs>
<p>bootstrap the ceph monitor</p>
</td><td class=code><div class=highlight><pre>
    sudo ceph-mon -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> --mkfs -i <span class="k">$(</span>hostname<span class="k">)</span> --keyring /var/lib/ceph/tmp/keyring.mon.<span class="k">$(</span>hostname<span class="k">)</span>
    <span class="k">if </span>is_ubuntu; <span class="k">then</span>
<span class="k">    </span>sudo touch /var/lib/ceph/mon/ceph-<span class="k">$(</span>hostname<span class="k">)</span>/upstart
        sudo initctl emit ceph-mon <span class="nv">id</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>
    <span class="k">else</span>
<span class="k">    </span>sudo touch /var/lib/ceph/mon/ceph-<span class="k">$(</span>hostname<span class="k">)</span>/sysvinit
        sudo service ceph start mon.<span class="k">$(</span>hostname<span class="k">)</span>
    <span class="k">fi</span>

</pre></div></td></tr><tr><td class=docs>
<p>wait for the admin key to come up otherwise we will not be able to do the actions below</p>
</td><td class=code><div class=highlight><pre>
    <span class="k">until</span> <span class="o">[</span> -f <span class="k">${</span><span class="nv">CEPH_CONF_DIR</span><span class="k">}</span>/ceph.client.admin.keyring <span class="o">]</span>; <span class="k">do</span>
<span class="k">        </span>echo_summary <span class="s2">&quot;Waiting for the Ceph admin key to be ready...&quot;</span>

        <span class="nv">count</span><span class="o">=</span><span class="k">$((</span><span class="nv">$count</span> <span class="o">+</span> <span class="m">1</span><span class="k">))</span>
        <span class="k">if</span> <span class="o">[</span> <span class="nv">$count</span> -eq 3 <span class="o">]</span>; <span class="k">then</span>
<span class="k">            </span>die <span class="nv">$LINENO</span> <span class="s2">&quot;Maximum of 3 retries reached&quot;</span>
        <span class="k">fi</span>
<span class="k">        </span>sleep 5
    <span class="k">done</span>

</pre></div></td></tr><tr><td class=docs>
<p>change pool replica size according to the CEPH_REPLICAS set by the user</p>
</td><td class=code><div class=highlight><pre>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set </span>data size <span class="k">${</span><span class="nv">CEPH_REPLICAS</span><span class="k">}</span>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set </span>rbd size <span class="k">${</span><span class="nv">CEPH_REPLICAS</span><span class="k">}</span>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set </span>metadata size <span class="k">${</span><span class="nv">CEPH_REPLICAS</span><span class="k">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>create a simple rule to take OSDs instead of host with CRUSH
then apply this rules to the default pool</p>
</td><td class=code><div class=highlight><pre>
    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$CEPH_REPLICAS</span> -ne 1 <span class="o">]]</span>; <span class="k">then</span>
<span class="k">        </span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd crush rule create-simple devstack default osd
        <span class="nv">RULE_ID</span><span class="o">=</span><span class="k">$(</span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd crush rule dump devstack | awk <span class="s1">&#39;/rule_id/ {print $3}&#39;</span> | cut -d <span class="s1">&#39;,&#39;</span> -f1<span class="k">)</span>
        sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set </span>rbd crush_ruleset <span class="k">${</span><span class="nv">RULE_ID</span><span class="k">}</span>
        sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set </span>data crush_ruleset <span class="k">${</span><span class="nv">RULE_ID</span><span class="k">}</span>
        sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set </span>metadata crush_ruleset <span class="k">${</span><span class="nv">RULE_ID</span><span class="k">}</span>
    <span class="k">fi</span>

</pre></div></td></tr><tr><td class=docs>
<p>create the OSD(s)</p>
</td><td class=code><div class=highlight><pre>
    <span class="k">for </span>rep in <span class="k">${</span><span class="nv">CEPH_REPLICAS_SEQ</span><span class="k">}</span>; <span class="k">do</span>
<span class="k">        </span><span class="nv">OSD_ID</span><span class="o">=</span><span class="k">$(</span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd create<span class="k">)</span>
        sudo mkdir -p <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span>/osd/ceph-<span class="k">${</span><span class="nv">OSD_ID</span><span class="k">}</span>
        sudo ceph-osd -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> -i <span class="k">${</span><span class="nv">OSD_ID</span><span class="k">}</span> --mkfs
        sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> auth get-or-create osd.<span class="k">${</span><span class="nv">OSD_ID</span><span class="k">}</span> mon <span class="s1">&#39;allow profile osd &#39;</span> osd <span class="s1">&#39;allow *&#39;</span> | sudo tee <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span>/osd/ceph-<span class="k">${</span><span class="nv">OSD_ID</span><span class="k">}</span>/keyring

</pre></div></td></tr><tr><td class=docs>
<p>ceph's init script is parsing ${CEPH_DATA_DIR}/osd/ceph-${OSD_ID}/ and looking for a file
'upstart' or 'sysinitv', thanks to these 'touches' we are able to control OSDs daemons
from the init script.</p>
</td><td class=code><div class=highlight><pre>
        <span class="k">if </span>is_ubuntu; <span class="k">then</span>
<span class="k">            </span>sudo touch <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span>/osd/ceph-<span class="k">${</span><span class="nv">OSD_ID</span><span class="k">}</span>/upstart
        <span class="k">else</span>
<span class="k">            </span>sudo touch <span class="k">${</span><span class="nv">CEPH_DATA_DIR</span><span class="k">}</span>/osd/ceph-<span class="k">${</span><span class="nv">OSD_ID</span><span class="k">}</span>/sysvinit
        <span class="k">fi</span>
<span class="k">    done</span>
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>configure_ceph_glance() - Glance config needs to come after Glance is set up</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>configure_ceph_glance <span class="o">{</span>
</pre></div></td></tr><tr><td class=docs>
<p>configure Glance service options, ceph pool, ceph user and ceph key</p>
</td><td class=code><div class=highlight><pre>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool create <span class="k">${</span><span class="nv">GLANCE_CEPH_POOL</span><span class="k">}</span> <span class="k">${</span><span class="nv">GLANCE_CEPH_POOL_PG</span><span class="k">}</span> <span class="k">${</span><span class="nv">GLANCE_CEPH_POOL_PGP</span><span class="k">}</span>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set</span> <span class="k">${</span><span class="nv">GLANCE_CEPH_POOL</span><span class="k">}</span> size <span class="k">${</span><span class="nv">CEPH_REPLICAS</span><span class="k">}</span>
    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$CEPH_REPLICAS</span> -ne 1 <span class="o">]]</span>; <span class="k">then</span>
<span class="k">        </span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set</span> <span class="k">${</span><span class="nv">GLANCE_CEPH_POOL</span><span class="k">}</span> crush_ruleset <span class="k">${</span><span class="nv">RULE_ID</span><span class="k">}</span>
    <span class="k">fi</span>
<span class="k">    </span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> auth get-or-create client.<span class="k">${</span><span class="nv">GLANCE_CEPH_USER</span><span class="k">}</span> mon <span class="s2">&quot;allow r&quot;</span> osd <span class="s2">&quot;allow class-read object_prefix rbd_children, allow rwx pool=${GLANCE_CEPH_POOL}&quot;</span> | sudo tee <span class="k">${</span><span class="nv">CEPH_CONF_DIR</span><span class="k">}</span>/ceph.client.<span class="k">${</span><span class="nv">GLANCE_CEPH_USER</span><span class="k">}</span>.keyring
    sudo chown <span class="k">${</span><span class="nv">STACK_USER</span><span class="k">}</span>:<span class="k">$(</span>id -g -n <span class="nv">$whoami</span><span class="k">)</span> <span class="k">${</span><span class="nv">CEPH_CONF_DIR</span><span class="k">}</span>/ceph.client.<span class="k">${</span><span class="nv">GLANCE_CEPH_USER</span><span class="k">}</span>.keyring
    iniset <span class="nv">$GLANCE_API_CONF</span> DEFAULT default_store rbd
    iniset <span class="nv">$GLANCE_API_CONF</span> DEFAULT rbd_store_ceph_conf <span class="nv">$CEPH_CONF_FILE</span>
    iniset <span class="nv">$GLANCE_API_CONF</span> DEFAULT rbd_store_user <span class="nv">$GLANCE_CEPH_USER</span>
    iniset <span class="nv">$GLANCE_API_CONF</span> DEFAULT rbd_store_pool <span class="nv">$GLANCE_CEPH_POOL</span>
    iniset <span class="nv">$GLANCE_API_CONF</span> DEFAULT show_image_direct_url True
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>configure_ceph_nova() - Nova config needs to come after Nova is set up</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>configure_ceph_nova <span class="o">{</span>
</pre></div></td></tr><tr><td class=docs>
<p>configure Nova service options, ceph pool, ceph user and ceph key</p>
</td><td class=code><div class=highlight><pre>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool create <span class="k">${</span><span class="nv">NOVA_CEPH_POOL</span><span class="k">}</span> <span class="k">${</span><span class="nv">NOVA_CEPH_POOL_PG</span><span class="k">}</span> <span class="k">${</span><span class="nv">NOVA_CEPH_POOL_PGP</span><span class="k">}</span>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set</span> <span class="k">${</span><span class="nv">NOVA_CEPH_POOL</span><span class="k">}</span> size <span class="k">${</span><span class="nv">CEPH_REPLICAS</span><span class="k">}</span>
    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$CEPH_REPLICAS</span> -ne 1 <span class="o">]]</span>; <span class="k">then</span>
<span class="k">        </span>sudo -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> ceph osd pool <span class="nb">set</span> <span class="k">${</span><span class="nv">NOVA_CEPH_POOL</span><span class="k">}</span> crush_ruleset <span class="k">${</span><span class="nv">RULE_ID</span><span class="k">}</span>
    <span class="k">fi</span>
<span class="k">    </span>iniset <span class="nv">$NOVA_CONF</span> libvirt rbd_user <span class="k">${</span><span class="nv">CINDER_CEPH_USER</span><span class="k">}</span>
    iniset <span class="nv">$NOVA_CONF</span> libvirt rbd_secret_uuid <span class="k">${</span><span class="nv">CINDER_CEPH_UUID</span><span class="k">}</span>
    iniset <span class="nv">$NOVA_CONF</span> libvirt inject_key <span class="nb">false</span>
<span class="nb">    </span>iniset <span class="nv">$NOVA_CONF</span> libvirt inject_partition -2
    iniset <span class="nv">$NOVA_CONF</span> libvirt disk_cachemodes <span class="s2">&quot;network=writeback&quot;</span>
    iniset <span class="nv">$NOVA_CONF</span> libvirt images_type rbd
    iniset <span class="nv">$NOVA_CONF</span> libvirt images_rbd_pool <span class="k">${</span><span class="nv">NOVA_CEPH_POOL</span><span class="k">}</span>
    iniset <span class="nv">$NOVA_CONF</span> libvirt images_rbd_ceph_conf <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span>
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>configure_ceph_cinder() - Cinder config needs to come after Cinder is set up</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>configure_ceph_cinder <span class="o">{</span>
</pre></div></td></tr><tr><td class=docs>
<p>Configure Cinder service options, ceph pool, ceph user and ceph key</p>
</td><td class=code><div class=highlight><pre>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool create <span class="k">${</span><span class="nv">CINDER_CEPH_POOL</span><span class="k">}</span> <span class="k">${</span><span class="nv">CINDER_CEPH_POOL_PG</span><span class="k">}</span> <span class="k">${</span><span class="nv">CINDER_CEPH_POOL_PGP</span><span class="k">}</span>
    sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set</span> <span class="k">${</span><span class="nv">CINDER_CEPH_POOL</span><span class="k">}</span> size <span class="k">${</span><span class="nv">CEPH_REPLICAS</span><span class="k">}</span>
    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$CEPH_REPLICAS</span> -ne 1 <span class="o">]]</span>; <span class="k">then</span>
<span class="k">        </span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd pool <span class="nb">set</span> <span class="k">${</span><span class="nv">CINDER_CEPH_POOL</span><span class="k">}</span> crush_ruleset <span class="k">${</span><span class="nv">RULE_ID</span><span class="k">}</span>

    <span class="k">fi</span>
<span class="k">    </span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> auth get-or-create client.<span class="k">${</span><span class="nv">CINDER_CEPH_USER</span><span class="k">}</span> mon <span class="s2">&quot;allow r&quot;</span> osd <span class="s2">&quot;allow class-read object_prefix rbd_children, allow rwx pool=${CINDER_CEPH_POOL}, allow rwx pool=${NOVA_CEPH_POOL},allow rx pool=${GLANCE_CEPH_POOL}&quot;</span> | sudo tee <span class="k">${</span><span class="nv">CEPH_CONF_DIR</span><span class="k">}</span>/ceph.client.<span class="k">${</span><span class="nv">CINDER_CEPH_USER</span><span class="k">}</span>.keyring
    sudo chown <span class="k">${</span><span class="nv">STACK_USER</span><span class="k">}</span>:<span class="k">$(</span>id -g -n <span class="nv">$whoami</span><span class="k">)</span> <span class="k">${</span><span class="nv">CEPH_CONF_DIR</span><span class="k">}</span>/ceph.client.<span class="k">${</span><span class="nv">CINDER_CEPH_USER</span><span class="k">}</span>.keyring
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>init_ceph() - Initialize databases, etc.</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>init_ceph <span class="o">{</span>
</pre></div></td></tr><tr><td class=docs>
<p>clean up from previous (possibly aborted) runs
make sure to kill all ceph processes first</p>
</td><td class=code><div class=highlight><pre>
    sudo pkill -f ceph-mon <span class="o">||</span> <span class="nb">true</span>
<span class="nb">    </span>sudo pkill -f ceph-osd <span class="o">||</span> <span class="nb">true</span>
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>install_ceph() - Collect source and prepare</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>install_ceph <span class="o">{</span>
</pre></div></td></tr><tr><td class=docs>
<dl class="docutils">
<dt>NOTE(dtroyer): At some point it'll be easier to test for unsupported distros,</dt>
<dd>leveraging the list in stack.sh</dd>
</dl>
</td><td class=code><div class=highlight><pre>
    <span class="k">if</span> <span class="o">[[</span> <span class="k">${</span><span class="nv">os_CODENAME</span><span class="k">}</span> <span class="o">=</span>~ trusty <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="k">${</span><span class="nv">os_CODENAME</span><span class="k">}</span> <span class="o">=</span>~ Schrödinger’sCat <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="k">${</span><span class="nv">os_CODENAME</span><span class="k">}</span> <span class="o">=</span>~ Heisenbug <span class="o">]]</span>; <span class="k">then</span>
<span class="k">        </span><span class="nv">NO_UPDATE_REPOS</span><span class="o">=</span>False
        install_package ceph
    <span class="k">else</span>
<span class="k">        </span>exit_distro_not_supported <span class="s2">&quot;Ceph since your distro doesn&#39;t provide (at least) the Firefly release. Please use Ubuntu Trusty or Fedora 19/20&quot;</span>
    <span class="k">fi</span>
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>start_ceph() - Start running processes, including screen</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>start_ceph <span class="o">{</span>
    <span class="k">if </span>is_ubuntu; <span class="k">then</span>
<span class="k">        </span>sudo initctl emit ceph-mon <span class="nv">id</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>
        <span class="k">for </span>id in <span class="k">$(</span>sudo ceph -c <span class="k">${</span><span class="nv">CEPH_CONF_FILE</span><span class="k">}</span> osd ls<span class="k">)</span>; <span class="k">do</span>
<span class="k">            </span>sudo start ceph-osd <span class="nv">id</span><span class="o">=</span><span class="k">${</span><span class="nv">id</span><span class="k">}</span>
        <span class="k">done</span>
<span class="k">    else</span>
<span class="k">        </span>sudo service ceph start
    <span class="k">fi</span>
<span class="o">}</span>

</pre></div></td></tr><tr><td class=docs>
<p>stop_ceph() - Stop running processes (non-screen)</p>
</td><td class=code><div class=highlight><pre>
<span class="k">function </span>stop_ceph <span class="o">{</span>
    <span class="k">if </span>is_ubuntu; <span class="k">then</span>
<span class="k">        </span>sudo service ceph-mon-all stop &gt; /dev/null 2&gt;&amp;1
        sudo service ceph-osd-all stop &gt; /dev/null 2&gt;&amp;1
    <span class="k">else</span>
<span class="k">        </span>sudo service ceph stop &gt; /dev/null 2&gt;&amp;1
    <span class="k">fi</span>
<span class="o">}</span>


</pre></div></td></tr><tr><td class=docs>
<p>Restore xtrace</p>
</td><td class=code><div class=highlight><pre>
<span class="nv">$XTRACE</span>

<span class="c">## Local variables:</span>
<span class="c">## mode: shell-script</span>
<span class="c">## End:</span>


</td><td class=code><div class=highlight><pre>
</pre></div></td></tr><tr><td class=docs>
</pre></div></td></tr><tr><td class=docs>
</td><td class=code><div class=highlight><pre>

</pre></div></td></tr><tr><td class=docs>
</div>
</div>
</body>
</html></td><td class='code'></td></tr>
    </tbody>
    </table>
</div>
</body>
</html>
